{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1g2EKyOXiY-CgQ0JWQoYEMDr_qXMV69zW","authorship_tag":"ABX9TyMrGgQ+7779aqWgwiMYJQIH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6sHmzBFcKm-K","executionInfo":{"status":"ok","timestamp":1695241389795,"user_tz":-180,"elapsed":1459,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"outputs":[],"source":["# importing required libraries\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import math\n","from tqdm import tqdm\n","import datetime\n","import re\n","from urllib.parse import urlparse\n","import urllib.parse\n","import time"]},{"cell_type":"code","source":["def scrape_github_pull_requests(pr_url):\n","    base_url = 'https://github.com'\n","\n","    # Initialize empty lists to store pull request titles and links\n","    pr_titles = []\n","    pr_links = []\n","\n","    # Initialize a variable to keep track of the page number\n","    page_number = 1\n","\n","    while True:\n","        # Construct the URL for the current page\n","        current_url = pr_url + f'?page={page_number}'\n","\n","        r = requests.get(current_url)\n","        if r.status_code != 200:\n","            break  # Break the loop if the page is not found\n","        else:\n","            soup = BeautifulSoup(r.text, 'html.parser')\n","\n","        # Find all anchor elements with the specified class and attributes for pull request links\n","        pr_links_on_page = soup.find_all('a', class_='Link--primary', attrs={'data-hovercard-type': 'pull_request'})\n","\n","        # Extract the title and link for each pull request and append them to the lists\n","        for link in pr_links_on_page:\n","            pr_title = link.text.strip()  # Get the text (title) inside the anchor tag\n","            pull_url = base_url + link['href']  # Get the href attribute (link)\n","\n","            pr_titles.append(pr_title)\n","            pr_links.append(pull_url)\n","\n","        # Check if there is a next page\n","        pagination = soup.find('div', {'class': 'pagination'})\n","        next_page = pagination.find('a', {'class': 'next_page'}) if pagination else None\n","        if next_page:\n","            page_number += 1\n","            time.sleep(2)\n","        else:\n","            break  # Exit the loop if there is no next page\n","\n","    return pr_titles, pr_links\n","\n","# # Example usage:\n","# pr_url = 'https://github.com/TheAlgorithms/Python/pulls'  # Repository's Pull Requests URL\n","# titles, links = scrape_github_pull_requests(pr_url)\n","# for title, link in zip(titles, links):\n","#     print(f'Title: {title}')\n","#     print(f'Link: {link}')"],"metadata":{"id":"XNKO5k2oMm0a","executionInfo":{"status":"ok","timestamp":1695241389796,"user_tz":-180,"elapsed":6,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def scrape_github_pull_requests_closed(pr_url):\n","    base_url = 'https://github.com'\n","\n","    # Initialize empty lists to store pull request titles and links\n","    pr_titles = []\n","    pr_links = []\n","\n","    # Initialize a variable to keep track of the page number\n","    page_number = 1\n","\n","    while True:\n","        # Construct the URL for the current page\n","        current_url = pr_url + f'?q=is%3Apr+is%3Aclosed&page={page_number}'\n","\n","        r = requests.get(current_url)\n","        if r.status_code != 200:\n","            break  # Break the loop if the page is not found\n","        else:\n","            soup = BeautifulSoup(r.text, 'html.parser')\n","\n","        # Find all anchor elements with the specified class and attributes for pull request links\n","        pr_links_on_page = soup.find_all('a', class_='Link--primary', attrs={'data-hovercard-type': 'pull_request'})\n","\n","        # Extract the title and link for each pull request and append them to the lists\n","        for link in pr_links_on_page:\n","            pr_title = link.text.strip()  # Get the text (title) inside the anchor tag\n","            pull_url = base_url + link['href']  # Get the href attribute (link)\n","\n","            pr_titles.append(pr_title)\n","            pr_links.append(pull_url)\n","\n","        # Check if there is a next page\n","        pagination = soup.find('div', {'class': 'pagination'})\n","        next_page = pagination.find('a', {'class': 'next_page'}) if pagination else None\n","        # print(current_url)\n","        if next_page:\n","            page_number += 1\n","            time.sleep(2)\n","        else:\n","            break  # Exit the loop if there is no next page\n","\n","    return pr_titles, pr_links , len(pr_links)\n","\n","# # Example usage:\n","# pr_url = 'https://github.com/TheAlgorithms/Python/pulls'  # Repository's Pull Requests URL\n","# titles, links , num = scrape_github_pull_requests_closed(pr_url)\n","# print(num)\n","# # for title, link in zip(titles, links):\n","# #     print(f'Title: {title}')\n","# #     print(f'Link: {link}')"],"metadata":{"id":"2dpnU8zWQkqy","executionInfo":{"status":"ok","timestamp":1695241389796,"user_tz":-180,"elapsed":5,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def scrape_github_issues(url):\n","    base_url = 'https://github.com'\n","\n","    # Initialize empty lists to store issue titles and links\n","    issue_titles = []\n","    issue_links = []\n","\n","    # Initialize a variable to keep track of the page number\n","    page_number = 1\n","\n","    while True:\n","        # Construct the URL for the current page\n","        current_url = url + f'?page={page_number}'\n","\n","        r = requests.get(current_url)\n","        if r.status_code != 200:\n","            break  # Break the loop if the page is not found\n","        else:\n","            soup = BeautifulSoup(r.text, 'html.parser')\n","\n","        # Find all anchor elements with the specified class\n","        issue_links_on_page = soup.find_all('a', class_='Link--primary')\n","\n","        # Extract the title and link for each issue and append them to the lists\n","        for link in issue_links_on_page:\n","            issue_title = link.text.strip()  # Get the text (title) inside the anchor tag\n","            issue_url = base_url + link['href']  # Get the href attribute (link)\n","\n","            issue_titles.append(issue_title)\n","            issue_links.append(issue_url)\n","\n","        # Check if there is a next page\n","        pagination = soup.find('div', {'class': 'pagination'})\n","        next_page = pagination.find('a', {'class': 'next_page'}) if pagination else None\n","        if next_page:\n","            page_number += 1\n","            time.sleep(2)\n","        else:\n","            break  # Exit the loop if there is no next page\n","\n","    return issue_titles, issue_links\n","\n","# # Example usage:\n","# pr_url = 'https://github.com/TheAlgorithms/Python/issues'\n","# issue_titles, issue_links = scrape_github_issues(pr_url)\n","# print(\"Issue Titles:\")\n","# print(issue_titles)\n","# print(\"\\nIssue Links:\")\n","# print(issue_links)"],"metadata":{"id":"rcXl8O_xRExN","executionInfo":{"status":"ok","timestamp":1695241390346,"user_tz":-180,"elapsed":5,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def scrape_github_issues_closed(url):\n","    base_url = 'https://github.com'\n","\n","    # Initialize empty lists to store issue titles and links\n","    issue_titles = []\n","    issue_links = []\n","\n","    # Initialize a variable to keep track of the page number\n","    page_number = 1\n","\n","    while True:\n","        # Construct the URL for the current page\n","        current_url = url + f'?q=is%3Aissue+is%3Aclosed&page={page_number}'\n","\n","        r = requests.get(current_url)\n","        if r.status_code != 200:\n","            break  # Break the loop if the page is not found\n","        else:\n","            soup = BeautifulSoup(r.text, 'html.parser')\n","\n","        # Find all anchor elements with the specified class\n","        issue_links_on_page = soup.find_all('a', class_='Link--primary')\n","\n","        # Extract the title and link for each issue and append them to the lists\n","        for link in issue_links_on_page:\n","            issue_title = link.text.strip()  # Get the text (title) inside the anchor tag\n","            issue_url = base_url + link['href']  # Get the href attribute (link)\n","\n","            issue_titles.append(issue_title)\n","            issue_links.append(issue_url)\n","\n","        # Check if there is a next page\n","        pagination = soup.find('div', {'class': 'pagination'})\n","        next_page = pagination.find('a', {'class': 'next_page'}) if pagination else None\n","        # print(current_url)\n","        if next_page:\n","            page_number += 1\n","            time.sleep(2)\n","        else:\n","            break  # Exit the loop if there is no next page\n","\n","    return issue_titles, issue_links , len(issue_links)\n","\n","# # Example usage:\n","# pr_url = 'https://github.com/TheAlgorithms/Python/issues'\n","# issue_titles, issue_links , num = scrape_github_issues_closed(pr_url)\n","# print(num)\n","# print(\"Issue Titles:\")\n","# print(issue_titles)\n","# print(\"\\nIssue Links:\")\n","# print(issue_links)"],"metadata":{"id":"48whrgHMTzLa","executionInfo":{"status":"ok","timestamp":1695241390346,"user_tz":-180,"elapsed":4,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def get_popular_repo_details(url):\n","\n","      # creating another dictionary to strore the data of popular repositories & their details\n","    pop_repo_details ={\n","       'PR_URL':[],'repo_name':[],'Repository_username':[],'link_profile_username':[],\n","       'Stars':[],'Forks':[],'Commits':[],'Last_committed':[] ,'issues_link':[],\n","       'num_issues_open':[],'num_issues_closed':[],'titles_issues_open':[],'links_issues_open':[],'titles_issues_closed':[],'links_issues_closed':[],\n","       'pull_requests_link':[],'num_pull_requests_open':[],'num_pull_requests_closed':[],'titles_pull_open':[],'links_pull_open':[],'titles_pull_closed':[],'links_pull_closed':[],}\n","\n","\n","\n","\n","    pr_urls = url    # repo URL\n","\n","    r =requests.get(pr_urls)\n","    if r.status_code != 200: i-=1\n","    else:\n","      pr_soup2 =BeautifulSoup(r.text, 'html.parser')\n","\n","    base_url = 'https://github.com/'\n","\n","\n","    # Parse the URL\n","    parsed_url = urlparse(pr_urls)\n","    # Split the path of the URL\n","    path_parts = parsed_url.path.split('/')\n","    # Extract user name and repository name\n","    user_name = path_parts[1]\n","    repo_name = path_parts[2]\n","    # Construct the user profile link\n","    user_profile_link = base_url + user_name\n","\n","\n","    # locating & extracting tags for star counts\n","    star_span_tag = pr_soup2.find_all('span',{'id':'repo-stars-counter-star'})\n","    stars = int(star_span_tag[0]['aria-label'].split()[0])\n","\n","    # locating & extracting tags for forks counts\n","    forks_span_tag =pr_soup2.find_all('span',{'id':'repo-network-counter'})\n","    forks = int(forks_span_tag[0]['title'].replace(',', ''))\n","\n","    # locating & extracting tags for commits\n","    commit_span_tags = pr_soup2.find_all('span',{'class':'d-none d-sm-inline'})\n","    commits = int(commit_span_tags[1].strong.text.replace(',', '')) if len(commit_span_tags)==2 else int(commit_span_tags[0].strong.text.replace(',', ''))\n","\n","    # locating & extracting tags for commits\n","    commit_span_tags = pr_soup2.find_all('span',{'class':'d-none d-sm-inline'})\n","    commits = int(commit_span_tags[1].strong.text.replace(',', '')) if len(commit_span_tags)==2 else int(commit_span_tags[0].strong.text.replace(',', ''))\n","\n","    time.sleep(2)\n","    # locating & extracting tags for last committed time\n","    last_commit_atag =pr_soup2.find_all('a',{'class':'Link--secondary ml-2'})\n","    last_updated = last_commit_atag[0].find_all('relative-time')[0]['datetime'] if len(last_commit_atag)>=1 else None\n","\n","\n","   # locating & extracting tags for issues counts\n","    issue_span_tag =pr_soup2.find_all('span',{'id':\"issues-repo-tab-count\"})\n","    issues = int(issue_span_tag[0]['title'])\n","\n","    # Locate and extract the <a> tag with the id 'issues-tab'\n","    issues_tab_a_tag = pr_soup2.find('a', {'id': 'issues-tab'})\n","    # Extract the 'href' attribute to get the link for issues\n","    issues_link = issues_tab_a_tag['href']\n","\n","    full_issues_link = base_url + issues_link\n","\n","    issue_titles_open, issue_links_open = scrape_github_issues(full_issues_link)\n","\n","    issue_titles_closed, issue_links_closed , num_issues_closed = scrape_github_issues_closed(full_issues_link)\n","\n","    time.sleep(2)\n","\n","    # locating & extracting tags for pull_requests counts\n","    pull_span_tag =pr_soup2.find_all('span',{'id':\"pull-requests-repo-tab-count\"})\n","    pull_requests= int(pull_span_tag[0]['title'])\n","\n","    # Locate and extract the <a> tag with the id 'pull_tab'\n","    pull_tab_a_tag = pr_soup2.find('a', {'id': 'pull-requests-tab'})\n","    # Extract the 'href' attribute to get the link for  pull_requests\n","    pull_requests_link = pull_tab_a_tag['href']\n","\n","    full_pull_requests_link = base_url + pull_requests_link\n","\n","\n","    titles_pull_open, links_pull_open = scrape_github_pull_requests(full_pull_requests_link)\n","\n","    titles_pull_closed, links_pull_closed , num_pull_requests_closed = scrape_github_pull_requests_closed(full_pull_requests_link)\n","\n","\n","    # appending scraped data for popular repository to the dictionary\n","    pop_repo_details['PR_URL'].append(pr_urls)\n","    pop_repo_details['repo_name'].append(repo_name)\n","    pop_repo_details['Repository_username'].append(user_name)\n","    pop_repo_details['link_profile_username'].append(user_profile_link)\n","    pop_repo_details['Stars'].append(stars)\n","    pop_repo_details['Forks'].append(forks)\n","    pop_repo_details['Commits'].append(commits)\n","    pop_repo_details['Last_committed'].append(last_updated)\n","\n","    # issues\n","    pop_repo_details['issues_link'].append(full_issues_link)\n","    pop_repo_details['num_issues_open'].append(issues)\n","    pop_repo_details['num_issues_closed'].append(num_issues_closed)\n","    pop_repo_details['titles_issues_open'].append(issue_titles_open)\n","    pop_repo_details['links_issues_open'].append(issue_links_open)\n","    pop_repo_details['titles_issues_closed'].append(issue_titles_closed)\n","    pop_repo_details['links_issues_closed'].append(issue_links_closed)\n","\n","    #pull_requests\n","    pop_repo_details['pull_requests_link'].append(full_pull_requests_link)\n","    pop_repo_details['num_pull_requests_open'].append(pull_requests)\n","    pop_repo_details['num_pull_requests_closed'].append(num_pull_requests_closed)\n","    pop_repo_details['titles_pull_open'].append(titles_pull_open)\n","    pop_repo_details['links_pull_open'].append(links_pull_open)\n","    pop_repo_details['titles_pull_closed'].append(titles_pull_closed)\n","    pop_repo_details['links_pull_closed'].append(links_pull_closed)\n","\n","\n","    return pop_repo_details\n","\n"],"metadata":{"id":"S1YbAmQeXHt4","executionInfo":{"status":"ok","timestamp":1695241390860,"user_tz":-180,"elapsed":4,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# df = get_popular_repo_details('https://github.com/ljpzzz/machinelearning')"],"metadata":{"id":"erQrslzwLE8t","executionInfo":{"status":"ok","timestamp":1695241391422,"user_tz":-180,"elapsed":3,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# df"],"metadata":{"id":"VViRhOP5LRKH","executionInfo":{"status":"ok","timestamp":1695241392107,"user_tz":-180,"elapsed":7,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Example list of repository URLs\n","repo_urls = [\n","    'https://github.com/PrithivirajDamodaran/Gramformer',\n","    'https://github.com/Show-Me-the-Code/python',\n","    'https://github.com/sebastianruder/NLP-progress',\n","    'https://github.com/explosion/sense2vec',\n","    'https://github.com/stanfordnlp/GloVe'\n","\n","]"],"metadata":{"id":"0N5G2vdCcSU2","executionInfo":{"status":"ok","timestamp":1695241392872,"user_tz":-180,"elapsed":4,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Define the function to get popular repo details for a list of URLs\n","def get_details_saving(url_list):\n","    # Initialize an empty list to store the results\n","    repo_details_list = []\n","\n","    # Iterate through the list of URLs and call get_popular_repo_details for each URL\n","    for url in url_list:\n","        repo_details = get_popular_repo_details(url)\n","        repo_details_list.append(repo_details)\n","\n","    # Convert the list of dictionaries to a DataFrame\n","    df = pd.DataFrame(repo_details_list)\n","\n","    # Extract values from lists in each column using the provided keys\n","    df['PR_URL'] = df['PR_URL'].apply(lambda x: x[0])\n","    df['repo_name'] = df['repo_name'].apply(lambda x: x[0])\n","    df['Repository_username'] = df['Repository_username'].apply(lambda x: x[0])\n","    df['link_profile_username'] = df['link_profile_username'].apply(lambda x: x[0])\n","    df['Stars'] = df['Stars'].apply(lambda x: x[0])\n","    df['Forks'] = df['Forks'].apply(lambda x: x[0])\n","    df['Commits'] = df['Commits'].apply(lambda x: x[0])\n","    df['Last_committed'] = df['Last_committed'].apply(lambda x: x[0])\n","    df['issues_link'] = df['issues_link'].apply(lambda x: x[0])\n","    df['num_issues_open'] = df['num_issues_open'].apply(lambda x: x[0])\n","    df['num_issues_closed'] = df['num_issues_closed'].apply(lambda x: x[0])\n","    df['titles_issues_open'] = df['titles_issues_open'].apply(lambda x: x[0])\n","    df['links_issues_open'] = df['links_issues_open'].apply(lambda x: x[0])\n","    df['titles_issues_closed'] = df['titles_issues_closed'].apply(lambda x: x[0])\n","    df['links_issues_closed'] = df['links_issues_closed'].apply(lambda x: x[0])\n","    df['pull_requests_link'] = df['pull_requests_link'].apply(lambda x: x[0])\n","    df['num_pull_requests_open'] = df['num_pull_requests_open'].apply(lambda x: x[0])\n","    df['num_pull_requests_closed'] = df['num_pull_requests_closed'].apply(lambda x: x[0])\n","    df['titles_pull_open'] = df['titles_pull_open'].apply(lambda x: x[0])\n","    df['links_pull_open'] = df['links_pull_open'].apply(lambda x: x[0])\n","    df['titles_pull_closed'] = df['titles_pull_closed'].apply(lambda x: x[0])\n","    df['links_pull_closed'] = df['links_pull_closed'].apply(lambda x: x[0])\n","\n","\n","    return df"],"metadata":{"id":"sbtkOnSvLS5M","executionInfo":{"status":"ok","timestamp":1695241394146,"user_tz":-180,"elapsed":9,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Record the start time\n","start_time = time.time()\n","\n","df = get_details_saving(repo_urls)\n","\n","# Record the end time\n","end_time = time.time()\n","# Calculate the runtime in seconds\n","runtime_seconds = end_time - start_time\n","\n","# Calculate the runtime in minutes\n","runtime_minutes = runtime_seconds / 60\n","\n","# Print the runtime in minutes\n","print(f\"Runtime: {runtime_minutes:.2f} minutes\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLnri9C9c3Lq","executionInfo":{"status":"ok","timestamp":1695241556009,"user_tz":-180,"elapsed":160625,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}},"outputId":"25a49703-0505-45a5-e7af-d8b16d36f6b3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Runtime: 2.67 minutes\n"]}]},{"cell_type":"code","source":["pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.width',100)"],"metadata":{"id":"8jw5U6zWdAxC","executionInfo":{"status":"ok","timestamp":1695241556010,"user_tz":-180,"elapsed":30,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"id":"HNbOVQzkdVv8","executionInfo":{"status":"ok","timestamp":1695241556011,"user_tz":-180,"elapsed":30,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}},"outputId":"8609f95d-009f-4c52-9ec6-e74e18cdf06e"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              PR_URL     repo_name   Repository_username  \\\n","0  https://github.com/PrithivirajDamodaran/Gramfo...    Gramformer  PrithivirajDamodaran   \n","1         https://github.com/Show-Me-the-Code/python        python      Show-Me-the-Code   \n","2     https://github.com/sebastianruder/NLP-progress  NLP-progress        sebastianruder   \n","3             https://github.com/explosion/sense2vec     sense2vec             explosion   \n","4               https://github.com/stanfordnlp/GloVe         GloVe           stanfordnlp   \n","\n","                     link_profile_username  Stars  Forks  Commits        Last_committed  \\\n","0  https://github.com/PrithivirajDamodaran   1390    163       73  2022-12-27T16:45:05Z   \n","1      https://github.com/Show-Me-the-Code   3633   2867     1499  2017-09-18T08:11:20Z   \n","2        https://github.com/sebastianruder  21891   3580      791  2023-03-09T10:36:42Z   \n","3             https://github.com/explosion   1532    241      460  2023-04-20T14:53:46Z   \n","4           https://github.com/stanfordnlp   6481   1475      174  2023-09-19T19:41:56Z   \n","\n","                                         issues_link  num_issues_open  num_issues_closed  \\\n","0  https://github.com//PrithivirajDamodaran/Gramf...                8                 15   \n","1  https://github.com//Show-Me-the-Code/python/is...               42                 18   \n","2  https://github.com//sebastianruder/NLP-progres...               35                 65   \n","3     https://github.com//explosion/sense2vec/issues               20                 91   \n","4       https://github.com//stanfordnlp/GloVe/issues               78                 81   \n","\n","                                  titles_issues_open  \\\n","0  [No module named 'annotated_text' in streamlit...   \n","1  [Python, Hey give me. Your. Name, rename files...   \n","2  [Tasks are not the right measure anymore, Depe...   \n","3  [provide citation, Train sense2vec in Chinese,...   \n","4  [there may be a bug in cooccur.c  when the bin...   \n","\n","                                   links_issues_open  \\\n","0  [https://github.com/PrithivirajDamodaran/Gramf...   \n","1  [https://github.com/Show-Me-the-Code/python/is...   \n","2  [https://github.com/sebastianruder/NLP-progres...   \n","3  [https://github.com/explosion/sense2vec/issues...   \n","4  [https://github.com/stanfordnlp/GloVe/issues/2...   \n","\n","                                titles_issues_closed  \\\n","0  [OSError: [E050] Can't find model 'en'. It doe...   \n","1  [Python3, 123, Code, Snap, P, Codr, Py, How to...   \n","2  [Add CFF (citation file format) to the reposit...   \n","3  [s2v standalone breaks if require_gpu() is cal...   \n","4  [utf-8 bug, Overflow in \"overflow_threshold\" (...   \n","\n","                                 links_issues_closed  \\\n","0  [https://github.com/PrithivirajDamodaran/Gramf...   \n","1  [https://github.com/Show-Me-the-Code/python/is...   \n","2  [https://github.com/sebastianruder/NLP-progres...   \n","3  [https://github.com/explosion/sense2vec/issues...   \n","4  [https://github.com/stanfordnlp/GloVe/issues/2...   \n","\n","                                  pull_requests_link  num_pull_requests_open  \\\n","0  https://github.com//PrithivirajDamodaran/Gramf...                       1   \n","1  https://github.com//Show-Me-the-Code/python/pulls                     108   \n","2  https://github.com//sebastianruder/NLP-progres...                      16   \n","3      https://github.com//explosion/sense2vec/pulls                       0   \n","4        https://github.com//stanfordnlp/GloVe/pulls                       2   \n","\n","   num_pull_requests_closed                                   titles_pull_open  \\\n","0                        12                                          [Spacy 3]   \n","1                       260  [Fake_Data_Generator, Add a Clue game that I l...   \n","2                       521  [Keyphrase Extraction and Generation for Engli...   \n","3                        48                                                 []   \n","4                        59  [Step size and gradient clipping for bias term...   \n","\n","                                     links_pull_open  \\\n","0  [https://github.com/PrithivirajDamodaran/Gramf...   \n","1  [https://github.com/Show-Me-the-Code/python/pu...   \n","2  [https://github.com/sebastianruder/NLP-progres...   \n","3                                                 []   \n","4  [https://github.com/stanfordnlp/GloVe/pull/209...   \n","\n","                                  titles_pull_closed  \\\n","0  [Change Topp to Beam search, updated banner, R...   \n","1  [Create hashes from a string, Create try, Crea...   \n","2  [Added introduction of Multimodal NLP, Add zer...   \n","3  [CI: Switch from Azure to GHA, Set version to ...   \n","4  [Make check-nan static to resolve undefined re...   \n","\n","                                   links_pull_closed  \n","0  [https://github.com/PrithivirajDamodaran/Gramf...  \n","1  [https://github.com/Show-Me-the-Code/python/pu...  \n","2  [https://github.com/sebastianruder/NLP-progres...  \n","3  [https://github.com/explosion/sense2vec/pull/1...  \n","4  [https://github.com/stanfordnlp/GloVe/pull/220...  "],"text/html":["\n","  <div id=\"df-ed7c3af8-8008-49fc-867c-90e436e01063\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PR_URL</th>\n","      <th>repo_name</th>\n","      <th>Repository_username</th>\n","      <th>link_profile_username</th>\n","      <th>Stars</th>\n","      <th>Forks</th>\n","      <th>Commits</th>\n","      <th>Last_committed</th>\n","      <th>issues_link</th>\n","      <th>num_issues_open</th>\n","      <th>num_issues_closed</th>\n","      <th>titles_issues_open</th>\n","      <th>links_issues_open</th>\n","      <th>titles_issues_closed</th>\n","      <th>links_issues_closed</th>\n","      <th>pull_requests_link</th>\n","      <th>num_pull_requests_open</th>\n","      <th>num_pull_requests_closed</th>\n","      <th>titles_pull_open</th>\n","      <th>links_pull_open</th>\n","      <th>titles_pull_closed</th>\n","      <th>links_pull_closed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>https://github.com/PrithivirajDamodaran/Gramfo...</td>\n","      <td>Gramformer</td>\n","      <td>PrithivirajDamodaran</td>\n","      <td>https://github.com/PrithivirajDamodaran</td>\n","      <td>1390</td>\n","      <td>163</td>\n","      <td>73</td>\n","      <td>2022-12-27T16:45:05Z</td>\n","      <td>https://github.com//PrithivirajDamodaran/Gramf...</td>\n","      <td>8</td>\n","      <td>15</td>\n","      <td>[No module named 'annotated_text' in streamlit...</td>\n","      <td>[https://github.com/PrithivirajDamodaran/Gramf...</td>\n","      <td>[OSError: [E050] Can't find model 'en'. It doe...</td>\n","      <td>[https://github.com/PrithivirajDamodaran/Gramf...</td>\n","      <td>https://github.com//PrithivirajDamodaran/Gramf...</td>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>[Spacy 3]</td>\n","      <td>[https://github.com/PrithivirajDamodaran/Gramf...</td>\n","      <td>[Change Topp to Beam search, updated banner, R...</td>\n","      <td>[https://github.com/PrithivirajDamodaran/Gramf...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>https://github.com/Show-Me-the-Code/python</td>\n","      <td>python</td>\n","      <td>Show-Me-the-Code</td>\n","      <td>https://github.com/Show-Me-the-Code</td>\n","      <td>3633</td>\n","      <td>2867</td>\n","      <td>1499</td>\n","      <td>2017-09-18T08:11:20Z</td>\n","      <td>https://github.com//Show-Me-the-Code/python/is...</td>\n","      <td>42</td>\n","      <td>18</td>\n","      <td>[Python, Hey give me. Your. Name, rename files...</td>\n","      <td>[https://github.com/Show-Me-the-Code/python/is...</td>\n","      <td>[Python3, 123, Code, Snap, P, Codr, Py, How to...</td>\n","      <td>[https://github.com/Show-Me-the-Code/python/is...</td>\n","      <td>https://github.com//Show-Me-the-Code/python/pulls</td>\n","      <td>108</td>\n","      <td>260</td>\n","      <td>[Fake_Data_Generator, Add a Clue game that I l...</td>\n","      <td>[https://github.com/Show-Me-the-Code/python/pu...</td>\n","      <td>[Create hashes from a string, Create try, Crea...</td>\n","      <td>[https://github.com/Show-Me-the-Code/python/pu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>https://github.com/sebastianruder/NLP-progress</td>\n","      <td>NLP-progress</td>\n","      <td>sebastianruder</td>\n","      <td>https://github.com/sebastianruder</td>\n","      <td>21891</td>\n","      <td>3580</td>\n","      <td>791</td>\n","      <td>2023-03-09T10:36:42Z</td>\n","      <td>https://github.com//sebastianruder/NLP-progres...</td>\n","      <td>35</td>\n","      <td>65</td>\n","      <td>[Tasks are not the right measure anymore, Depe...</td>\n","      <td>[https://github.com/sebastianruder/NLP-progres...</td>\n","      <td>[Add CFF (citation file format) to the reposit...</td>\n","      <td>[https://github.com/sebastianruder/NLP-progres...</td>\n","      <td>https://github.com//sebastianruder/NLP-progres...</td>\n","      <td>16</td>\n","      <td>521</td>\n","      <td>[Keyphrase Extraction and Generation for Engli...</td>\n","      <td>[https://github.com/sebastianruder/NLP-progres...</td>\n","      <td>[Added introduction of Multimodal NLP, Add zer...</td>\n","      <td>[https://github.com/sebastianruder/NLP-progres...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>https://github.com/explosion/sense2vec</td>\n","      <td>sense2vec</td>\n","      <td>explosion</td>\n","      <td>https://github.com/explosion</td>\n","      <td>1532</td>\n","      <td>241</td>\n","      <td>460</td>\n","      <td>2023-04-20T14:53:46Z</td>\n","      <td>https://github.com//explosion/sense2vec/issues</td>\n","      <td>20</td>\n","      <td>91</td>\n","      <td>[provide citation, Train sense2vec in Chinese,...</td>\n","      <td>[https://github.com/explosion/sense2vec/issues...</td>\n","      <td>[s2v standalone breaks if require_gpu() is cal...</td>\n","      <td>[https://github.com/explosion/sense2vec/issues...</td>\n","      <td>https://github.com//explosion/sense2vec/pulls</td>\n","      <td>0</td>\n","      <td>48</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[CI: Switch from Azure to GHA, Set version to ...</td>\n","      <td>[https://github.com/explosion/sense2vec/pull/1...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>https://github.com/stanfordnlp/GloVe</td>\n","      <td>GloVe</td>\n","      <td>stanfordnlp</td>\n","      <td>https://github.com/stanfordnlp</td>\n","      <td>6481</td>\n","      <td>1475</td>\n","      <td>174</td>\n","      <td>2023-09-19T19:41:56Z</td>\n","      <td>https://github.com//stanfordnlp/GloVe/issues</td>\n","      <td>78</td>\n","      <td>81</td>\n","      <td>[there may be a bug in cooccur.c  when the bin...</td>\n","      <td>[https://github.com/stanfordnlp/GloVe/issues/2...</td>\n","      <td>[utf-8 bug, Overflow in \"overflow_threshold\" (...</td>\n","      <td>[https://github.com/stanfordnlp/GloVe/issues/2...</td>\n","      <td>https://github.com//stanfordnlp/GloVe/pulls</td>\n","      <td>2</td>\n","      <td>59</td>\n","      <td>[Step size and gradient clipping for bias term...</td>\n","      <td>[https://github.com/stanfordnlp/GloVe/pull/209...</td>\n","      <td>[Make check-nan static to resolve undefined re...</td>\n","      <td>[https://github.com/stanfordnlp/GloVe/pull/220...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed7c3af8-8008-49fc-867c-90e436e01063')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ed7c3af8-8008-49fc-867c-90e436e01063 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ed7c3af8-8008-49fc-867c-90e436e01063');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bd25c22e-e2ea-471e-874b-12114624db35\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd25c22e-e2ea-471e-874b-12114624db35')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bd25c22e-e2ea-471e-874b-12114624db35 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Save the DataFrame to a CSV file\n","df.to_csv('/content/drive/MyDrive/hamza/repos_details.csv', index=False)"],"metadata":{"id":"DUO3npUqdYO6","executionInfo":{"status":"ok","timestamp":1695241699500,"user_tz":-180,"elapsed":577,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dWx2Yjw-hQrW","executionInfo":{"status":"ok","timestamp":1695241001019,"user_tz":-180,"elapsed":59,"user":{"displayName":"Mahmoud Soroor","userId":"03557047343890473665"}}},"execution_count":14,"outputs":[]}]}